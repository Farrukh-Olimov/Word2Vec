{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import math\n",
    "from tensorflow import nn\n",
    "from collections import Counter\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def subsample(data):\n",
    "    threshold=1e-5\n",
    "    data=data.split()\n",
    "    data=data[:len(data)//10]\n",
    "    word_count=Counter(data)\n",
    "    total_count=len(data)\n",
    "    freq={word:count/total_count for word,count in word_count.items()}\n",
    "    p_drop={word:1-np.sqrt(threshold/freq[word]) for word in word_count}\n",
    "    subsampled=[word for word in data if random.random() <1 -p_drop[word]]\n",
    "    return subsampled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_dictionary(data):\n",
    "    words=set(data) ## gets unique words only\n",
    "    word2int={}\n",
    "    int2word={}\n",
    "    vocab_size=len(words)\n",
    "    \n",
    "    for i, word in enumerate(words):\n",
    "        word2int[word]=i\n",
    "        int2word[i]=word\n",
    "    return word2int,int2word,vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17005207\n",
      "478894\n"
     ]
    }
   ],
   "source": [
    "def read_data(filename):\n",
    "    with open(filename,'r') as f:\n",
    "        return f.read()\n",
    "\n",
    "url='/home/farrukh/Courses GIST/Codes and Books/Word2Vec/datasets/Word2Vec/text8'\n",
    "data=read_data(url)\n",
    "subsampled=subsample(data)\n",
    "print(len(data.split()))\n",
    "print(len(subsampled))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "253854\n"
     ]
    }
   ],
   "source": [
    "word2int,int2word,vocabulari_size=make_dictionary(data.split())\n",
    "embedding_size=128\n",
    "print(vocabulari_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "## used for checking and debuggin \n",
    "#data='the quick brown fox jumped over the lazy dog'\n",
    "#word2int,int2word,vocabulari_size=make_dictionary(data)\n",
    "#embedding_size=5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "70889"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(word2int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1700518\n"
     ]
    }
   ],
   "source": [
    "## makes stacked (context,target) tuples using window size of 1\n",
    "def make_context_target_pairs(data,window_size=1):\n",
    "    dataset=data\n",
    "    left=[dataset[i-window_size:i] for i in range(window_size,len(dataset)-window_size)]\n",
    "    right=[dataset[i+1:i+window_size+1] for i in range(window_size,len(dataset)-window_size)]\n",
    "    middle=[dataset[i] for i in range(window_size,len(dataset)-window_size)]\n",
    "    dataset=[]\n",
    "    for i in range(len(left)):\n",
    "        combined=left[i]\n",
    "        combined.extend(right[i])\n",
    "        dataset.append((combined,middle[i]))\n",
    "    return dataset\n",
    "\n",
    "#makes unstacked (target_i,context_i_j) pairwise tuples\n",
    "## where (target,context) is the same as (input,output) datasets for skip-grammar model\n",
    "def make_pair(data):\n",
    "    dataset=[]\n",
    "    for i in range(len(data)):\n",
    "        dataset.extend([[data[i][1],data[i][0][j]] for j in range(len(data[i][0])) ])\n",
    "    return dataset\n",
    "lngth=len(data.split())\n",
    "preprocessed=make_context_target_pairs(data.split()[:lngth//10],1)\n",
    "invrtd_data=make_pair(preprocessed)\n",
    "preprocessed,invrtd_data\n",
    "print(len(preprocessed))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[167993,  10207],\n",
       "       [167993, 137436],\n",
       "       [137436, 167993],\n",
       "       ...,\n",
       "       [103693, 227854],\n",
       "       [227854, 103693],\n",
       "       [227854,  12372]])"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def replace_data_with_codes(data,word2int):\n",
    "    dataset=[[word2int[inpt],word2int[outpt]] for inpt,outpt in data]\n",
    "    return dataset\n",
    "train_data=np.array(replace_data_with_codes(invrtd_data,word2int))\n",
    "train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings=tf.Variable(tf.random_uniform([vocabulari_size,embedding_size],-1.0,1.0))\n",
    "nce_weights=tf.Variable(tf.truncated_normal([vocabulari_size,embedding_size]))\n",
    "nce_biases=tf.Variable(tf.zeros([vocabulari_size]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "norm=tf.sqrt(tf.reduce_sum(tf.square(embeddings),1,keepdims=True))\n",
    "normalized_embeddings=embeddings/norm\n",
    "#valid_embeddings=tf.nn.embedding_lookup(normalized_embeddings,valid_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size=10000\n",
    "train_input=tf.placeholder(tf.int32,shape=[batch_size])\n",
    "train_labels=tf.placeholder(tf.int32,shape=[batch_size,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed=tf.nn.embedding_lookup(embeddings,train_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss=tf.reduce_mean(nn.sampled_softmax_loss(weights=nce_weights,\n",
    "                                  biases=nce_biases,\n",
    "                                  labels=train_labels,\n",
    "                                  inputs=embed,\n",
    "                                  num_classes=vocabulari_size,\n",
    "                                  num_sampled=5,partition_strategy='div'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer=tf.train.GradientDescentOptimizer(learning_rate=1.0)\n",
    "training_op=optimizer.minimize(loss)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_batch(data,batch_size):\n",
    "    indecis=np.random.permutation(len(data))\n",
    "    n_batches=len(data)//batch_size\n",
    "    for ind in np.array_split(indecis,n_batches):\n",
    "        return data[ind[:batch_size],:1].reshape(batch_size), data[ind[:batch_size],1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3401036"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.56302524  0.787374   -0.9346087  ...  0.6997583  -0.9952462\n",
      "  -0.6744714 ]\n",
      " [ 0.03886008  0.69121027 -0.14256096 ...  0.27742195  0.28424072\n",
      "  -0.0429678 ]\n",
      " [ 0.8495772   0.93993044 -0.82224655 ...  0.72667575  0.85389876\n",
      "   0.53896046]\n",
      " ...\n",
      " [-0.6574106  -0.64742184  0.7485759  ... -0.98803973  0.16143823\n",
      "   0.8789139 ]\n",
      " [-0.80495524  0.9255545  -0.9140315  ... -0.5067208   0.7236531\n",
      "   0.38112855]\n",
      " [-0.9307692  -0.59663033  0.05232549 ... -0.85570717 -0.71479464\n",
      "   0.30171013]] \n",
      "\n",
      "\n",
      "Epoch: 0 out of 10000\n",
      "Epoch: 10 out of 10000\n",
      "Epoch: 20 out of 10000\n",
      "Epoch: 30 out of 10000\n",
      "Epoch: 40 out of 10000\n",
      "Epoch: 50 out of 10000\n",
      "Epoch: 60 out of 10000\n",
      "Epoch: 70 out of 10000\n",
      "Epoch: 80 out of 10000\n",
      "Epoch: 90 out of 10000\n",
      "Epoch: 100 out of 10000\n",
      "Epoch: 110 out of 10000\n",
      "Epoch: 120 out of 10000\n",
      "Epoch: 130 out of 10000\n",
      "Epoch: 140 out of 10000\n",
      "Epoch: 150 out of 10000\n",
      "Epoch: 160 out of 10000\n",
      "Epoch: 170 out of 10000\n",
      "Epoch: 180 out of 10000\n",
      "Epoch: 190 out of 10000\n",
      "Epoch: 200 out of 10000\n",
      "Epoch: 210 out of 10000\n",
      "Epoch: 220 out of 10000\n",
      "Epoch: 230 out of 10000\n",
      "Epoch: 240 out of 10000\n",
      "Epoch: 250 out of 10000\n",
      "Epoch: 260 out of 10000\n",
      "Epoch: 270 out of 10000\n",
      "Epoch: 280 out of 10000\n",
      "Epoch: 290 out of 10000\n",
      "Epoch: 300 out of 10000\n",
      "Epoch: 310 out of 10000\n",
      "Epoch: 320 out of 10000\n",
      "Epoch: 330 out of 10000\n",
      "Epoch: 340 out of 10000\n",
      "Epoch: 350 out of 10000\n",
      "Epoch: 360 out of 10000\n",
      "Epoch: 370 out of 10000\n",
      "Epoch: 380 out of 10000\n",
      "Epoch: 390 out of 10000\n",
      "Epoch: 400 out of 10000\n",
      "Epoch: 410 out of 10000\n",
      "Epoch: 420 out of 10000\n",
      "Epoch: 430 out of 10000\n",
      "Epoch: 440 out of 10000\n",
      "Epoch: 450 out of 10000\n",
      "Epoch: 460 out of 10000\n",
      "Epoch: 470 out of 10000\n",
      "Epoch: 480 out of 10000\n",
      "Epoch: 490 out of 10000\n",
      "Epoch: 500 out of 10000\n",
      "Epoch: 510 out of 10000\n",
      "Epoch: 520 out of 10000\n",
      "Epoch: 530 out of 10000\n",
      "Epoch: 540 out of 10000\n",
      "Epoch: 550 out of 10000\n",
      "Epoch: 560 out of 10000\n",
      "Epoch: 570 out of 10000\n",
      "Epoch: 580 out of 10000\n",
      "Epoch: 590 out of 10000\n",
      "Epoch: 600 out of 10000\n",
      "Epoch: 610 out of 10000\n",
      "Epoch: 620 out of 10000\n",
      "Epoch: 630 out of 10000\n",
      "Epoch: 640 out of 10000\n",
      "Epoch: 650 out of 10000\n",
      "Epoch: 660 out of 10000\n"
     ]
    }
   ],
   "source": [
    "init=tf.global_variables_initializer()\n",
    "saver=tf.train.Saver()\n",
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    print(embeddings.eval(),'\\n\\n')\n",
    "\n",
    "    for epoch in range(10000):\n",
    "        for i in range(len(train_data)//batch_size):\n",
    "            x_batch,y_batch=generate_batch(train_data,batch_size)\n",
    "            feed_dict={train_input:x_batch,train_labels:y_batch}\n",
    "            _,cur_loss=sess.run([training_op,loss], feed_dict=feed_dict)\n",
    "        if(epoch%10==0):\n",
    "            print('Epoch: %d out of 10000'%epoch)\n",
    "            saver.save(sess,'/home/farrukh/Courses GIST/Codes and Books/Word2Vec/datasets/Word2Vec/negModelNoSubsample.ckpt')\n",
    "    print(embeddings.eval(),'\\n\\n\\n\\n')\n",
    "    final_embeddings=normalized_embeddings.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.manifold import TSNE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_with_labels(low_dim_embs,labels):\n",
    "    plt.figure(figsize=(35,35))\n",
    "    for i, label in enumerate(labels):\n",
    "        x,y=low_dim_embs[i,:]\n",
    "        plt.scatter(x,y)\n",
    "        plt.annotate(label,\n",
    "                   xy=(x,y),\n",
    "                   xytext=(5,2),\n",
    "                   textcoords='offset points',\n",
    "                   ha='right',\n",
    "                   va='bottom')\n",
    "    plt.savefig('/home/farrukh/Courses GIST/Codes and Books/Word2Vec/datasets/Word2Vec/pic2.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tsne=TSNE(perplexity=32,n_components=2,init='pca',n_iter=5000,\n",
    "         method='exact')\n",
    "plot_only=1500\n",
    "low_dim_embs=tsne.fit_transform(final_embeddings[:plot_only,:])\n",
    "labels=[int2word[i] for i in range(plot_only)]\n",
    "plot_with_labels(low_dim_embs,labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
